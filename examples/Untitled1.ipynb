{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprised-apache",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advanced-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from TFBT import TFBT\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from urllib.request import urlretrieve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-detective",
   "metadata": {},
   "source": [
    "# Importing the dataset\n",
    "For this problem, I considered the CoverType dataset. It has seven classes and 581012 instances, and 54 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reported-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/datasets/covertype/covertype.csv'\n",
    "urlretrieve(url, 'covertype.csv')\n",
    "df = pd.read_csv('covertype.csv')\n",
    "X = (df.drop('Cover_Type', axis=1)).values\n",
    "y = (df['Cover_Type']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-mailing",
   "metadata": {},
   "source": [
    "# Splitting the dataset for the evaluation purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "removable-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(np.unique(y)-1)):\n",
    "    y[:][y[:] == i+1] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-immune",
   "metadata": {},
   "source": [
    "# Splitting the dataset for the evaluation purpose\n",
    "Due to the size of the dataset and calculation time, I did not consider h-params tunning. The h-params here are the optimal values, based on the previous studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "musical-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-gardening",
   "metadata": {},
   "source": [
    "# Training the models\n",
    "The purpose of this training is to have a comparison between two gradient boosting models. The important thing in this comparison is that the [TFBT](https://www.tensorflow.org/tutorials/estimator/boosted_trees) model had made over the XGboost logic.\n",
    "\n",
    "Also, I modified the TFBT here based on the Sklearn Standard. You may check the [class](https://github.com/samanemami/TFBoostedTree/blob/main/TFBT.py) here.\n",
    "\n",
    "About the Boosting iteration, I set both to 10, so we can compare the performance of both methods at the early stage of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unlimited-fetish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Sami\\\\Desktop\\\\TFBTtemp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\Sami\\Desktop\\TFBTtemp\\model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 1.9459187, step = 0\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 6...\n",
      "INFO:tensorflow:Saving checkpoints for 6 into C:\\Users\\Sami\\Desktop\\TFBTtemp\\model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 6...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\n",
      "INFO:tensorflow:Saving checkpoints for 10 into C:\\Users\\Sami\\Desktop\\TFBTtemp\\model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\n",
      "INFO:tensorflow:Loss for final step: 1.1915693.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-05-05T00:27:51Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Sami\\Desktop\\TFBTtemp\\model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Inference Time : 36.96717s\n",
      "INFO:tensorflow:Finished evaluation at 2021-05-05-00:28:28\n",
      "INFO:tensorflow:Saving dict for global step 10: accuracy = 0.58606803, average_loss = 1.159834, global_step = 10, loss = 1.159834\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: C:\\Users\\Sami\\Desktop\\TFBTtemp\\model.ckpt-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sami\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:28:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "tfbt = TFBT(n_batches_per_layer=1,\n",
    "            label_vocabulary=None,\n",
    "            n_trees=10,\n",
    "            max_depth=1,\n",
    "            learning_rate=0.1,\n",
    "            model_dir=None)\n",
    "pipe_tfbt = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", tfbt)])\n",
    "t0 = time.time()\n",
    "pipe_tfbt.fit(x_train, y_train)\n",
    "t1 = time.time()\n",
    "fit_ti_tfbt = t1-t0\n",
    "err_tfbt = pipe_tfbt.score(x_test, y_test)\n",
    "\n",
    "xgb = xgboost.XGBClassifier(learning_rate=0.5,\n",
    "                            max_depth=2,\n",
    "                            n_estimators=10,\n",
    "                            subsample=0.75,\n",
    "                            min_child_weight=1)\n",
    "pipe_xgb = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", xgb)])\n",
    "x0 = time.time()\n",
    "pipe_xgb.fit(x_train, y_train)\n",
    "x1 = time.time()\n",
    "fit_ti_xgb = x1-x0\n",
    "err_xgb = pipe_xgb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-trick",
   "metadata": {},
   "source": [
    "# Comparing the results\n",
    "For the comparison, I considered the accuracy of the classification and the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "southern-character",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TFBT accuracy</th>\n",
       "      <th>XGBoost accuracy</th>\n",
       "      <th>TFBT Training time</th>\n",
       "      <th>XGBoost Training time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <td>0.586068</td>\n",
       "      <td>0.706346</td>\n",
       "      <td>1404.223077</td>\n",
       "      <td>17.875942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TFBT accuracy  XGBoost accuracy  TFBT Training time  \\\n",
       "Values       0.586068          0.706346         1404.223077   \n",
       "\n",
       "        XGBoost Training time  \n",
       "Values              17.875942  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "result['TFBT accuracy'] = err_tfbt\n",
    "result['XGBoost accuracy'] = err_xgb\n",
    "result['TFBT Training time'] = fit_ti_tfbt\n",
    "result['XGBoost Training time'] = fit_ti_xgb\n",
    "pd.DataFrame(result, index=[\"Values\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6Test",
   "language": "python",
   "name": "python3.6test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
